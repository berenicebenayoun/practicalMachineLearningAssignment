x1 - x2 + c(-1,1)*qt(0.975,n1 + n2 -2)*sqrt(spsq*(1/n1 + 1/n2))
x2 - x1 + c(-1,1)*qt(0.975,n1 + n2 -2)*sqrt(spsq*(1/n1 + 1/n2))
c(-1,1)*qt(0.975,n1 + n2 -2)*sqrt(spsq*(1/n1 + 1/n2))
qt(0.975,n1 + n2 -2)*
1
n1 <- n2 <- 9
x1 <- -3  ##treated
x2 <- 1  ##placebo
s1 <- 1.5  ##treated
s2 <- 1.8  ##placebo
spsq <- ( (n1 - 1) * s1^2 + (n2 - 1) * s2^2) / (n1 + n2 - 2)
x1 - x2 + c(-1,1)*qt(0.95,16)*sqrt(spsq*(2/9))
x1 - x2 + c(-1,1)*qt(0.95,n1 + n2 -2)*sqrt(spsq*(1/n1 + 1/n2))
sqrt(  (9*0.6^2 + 9* 0.68^2) / (18)   )
n1 <- n2 <- 10
x1 <- 3  ## new
x2 <- 5  ##old
s1 <- 0.6  ##new
s2 <- 0.68  ##old
spsq <- ( (n1 - 1) * s1^2 + (n2 - 1) * s2^2) / (n1 + n2 - 2)
sqrt(spsq)
x1 - x2 + c(-1,1)*qt(0.975,n1 + n2 -2)*sqrt(0.5*(0.6+0.68)*(1/n1 + 1/n2))
knit2pdf
install.packages('rmarkdown')
render("/Users/benayoun/Dropbox/Coursera_data_science/rmd_stat_inference/07_Asymptopia.Rmd", "/Users/benayoun/Dropbox/Coursera_data_science/rmd_stat_inference/07_Asymptopia.pdf")
library(rmarkdown)
render("/Users/benayoun/Dropbox/Coursera_data_science/rmd_stat_inference/07_Asymptopia.Rmd", "/Users/benayoun/Dropbox/Coursera_data_science/rmd_stat_inference/07_Asymptopia.pdf")
install.packages(pandoc)
install.packages("pandoc")
my.data <- read.table("/Volumes/MyBook_2/Genome_suite/Annotations_MAKER/August6_Maker_gene_models_REPEATS_full_match.gff3",header=F,sep="\t")
head(my.data)
my.length <- my.data$V5-my.data$V4
hist(my.length)
sum(my.length)
sum(my.length)/1023205147
z.test
mean(mtcars$mpg)
qnorm(0.05)
sd(mtcars$mpg)
qnorm(0.05)*sd(mtcars$mpg)/sqrt(length(mtcars$mpg))  + mean(mtcars$mpg)
-qnorm(0.05)*sd(mtcars$mpg)/sqrt(length(mtcars$mpg))  + mean(mtcars$mpg)
my.4 <- which(mtcars$cyl == 4)
my.6 <- which(mtcars$cyl == 6)
t.test(mtcars$mpg[my.4],mtcars$mpg[my.6])
qnorm(0.975)
3 + c(-1,1)*qnorm(0.975)*1.1
3 + c(-1,1)*qnorm(0.025)*1.1
3 + c(-1,1)*qnorm(0.975)*1.1/sqrt(100)
?pbinom
pbinom(55,100,0.5)
pbinom(55,100,0.5,lower.tail = FALSE)
pbinom(54,100,0.5,lower.tail = FALSE)
?pois
?ppois
ppois(30/15800,520,lower.tail=False)
ppois(30/15800,520,lower.tail=F)
ppois(15800,520*30,lower.tail=F)
ppois(15800-1,520*30,lower.tail=F)
bas <- c(140,138,150,148,135)
treat <- c(132,135,151,146,130)
t.test(bas,treat,paired=T)
1100+c(-1,1)*30/sqrt(9)
1100+c(-1,1)*qnorm(0.975)*30/sqrt(9)
1100+c(-1,1)*qnorm(0.95)*30/sqrt(9)
1100+c(-1,1)*qt(0.975)*30/sqrt(9)
1100+c(-1,1)*qt(0.975,df=8)*30/sqrt(9)
1100+c(-1,1)*qt(0.975,df=8)*30
mean(bas)
mean(treat)
t.test(bas,treat)
t.test(bas)
sd(bas)
mean(bas)+c(-1,1)*qt(0.975,df=4)*sd(bas)/sqrt(5)
1100+c(-1,1)*qt(0.975,df=8)*30/sqrt(9)
?pbino
?pbinom
pbinom(2,4,0.5,lower.tail = FALSE)
ppois(10,0.01*1787)
sp <- sqrt( ( (9-1)*1.5^2 + (9-1)*1.8^2 )/(9+9-2)  )
sp
pt(4.419)
(-3-1)/(sp * sqrt(1/9+1/9))
pt(-5.121475)
pt(-5.121475,df=17)
pt(5.121475,df=17)
(1+3)//(sp * sqrt(1/9+1/9))
(1+3)/(sp * sqrt(1/9+1/9))
qnorm(0.95)
n <- 100
qnorm(0.95)*0.04/sqrt(n)
pnorm(0.006579415, mean = 0.01,sd=0.04/sqrt(n),lower.tail=F)
n <- 100
pnorm(qnorm(0.95)*0.04/sqrt(n), mean = 0.01,sd=0.04/sqrt(n),lower.tail=F)
n<-99
n <- 100
pnorm(qnorm(0.95)*0.04/sqrt(n), mean = 0.01,sd=0.04/sqrt(n),lower.tail=F)
n<-99
pnorm(qnorm(0.95)*0.04/sqrt(n), mean = 0.01,sd=0.04/sqrt(n),lower.tail=F)
n<-1000
pnorm(qnorm(0.95)*0.04/sqrt(n), mean = 0.01,sd=0.04/sqrt(n),lower.tail=F)
n<-200
pnorm(qnorm(0.95)*0.04/sqrt(n), mean = 0.01,sd=0.04/sqrt(n),lower.tail=F)
n<-150
pnorm(qnorm(0.95)*0.04/sqrt(n), mean = 0.01,sd=0.04/sqrt(n),lower.tail=F)
n<-140
pnorm(qnorm(0.95)*0.04/sqrt(n), mean = 0.01,sd=0.04/sqrt(n),lower.tail=F)
n<-135
pnorm(qnorm(0.95)*0.04/sqrt(n), mean = 0.01,sd=0.04/sqrt(n),lower.tail=F)
n<-137
pnorm(qnorm(0.95)*0.04/sqrt(n), mean = 0.01,sd=0.04/sqrt(n),lower.tail=F)
n<-138
pnorm(qnorm(0.95)*0.04/sqrt(n), mean = 0.01,sd=0.04/sqrt(n),lower.tail=F)
pnorm(qnorm(0.9)*0.04/sqrt(n), mean = 0.01,sd=0.04/sqrt(n),lower.tail=F)
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
plot(mixtures$CompressiveStrength)
colnames(mixtures)
summary(mixtures)
summary(mixtures,type="l")
plot(mixtures$CompressiveStrength,type='l')
plot(mixtures$CompressiveStrength,mixtures$FlyAsh)
plot(mixtures$CompressiveStrength,mixtures$Age)
length(mixtures$CompressiveStrength)
bla<-1:length(mixtures$CompressiveStrength)
plot(bla,mixtures$CompressiveStrength)
plot(bla[mixtures$Age < 5],mixtures$CompressiveStrength[mixtures$Age < 5])
plot(bla,mixtures$CompressiveStrength)
points(bla[mixtures$Age < 5],mixtures$CompressiveStrength[mixtures$Age < 5],col="red",pch=16)
points(bla[mixtures$Age > 40],mixtures$CompressiveStrength[mixtures$Age >40],col="blue",pch=16)
plot(bla,mixtures$CompressiveStrength)
points(bla[mixtures$Age < 5],mixtures$CompressiveStrength[mixtures$Age < 5],col="red",pch=16)
points(bla[mixtures$Age > 30],mixtures$CompressiveStrength[mixtures$Age >30],col="blue",pch=16)
points(bla[mixtures$Age > 50],mixtures$CompressiveStrength[mixtures$Age >50],col="gold",pch=16)
plot(bla,mixtures$CompressiveStrength)
points(bla[mixtures$Age < 5],mixtures$CompressiveStrength[mixtures$Age < 5],col="red",pch=16)
points(bla[mixtures$Age > 30],mixtures$CompressiveStrength[mixtures$Age >30],col="blue",pch=16)
points(bla[mixtures$Age > 50],mixtures$CompressiveStrength[mixtures$Age >60],col="gold",pch=16)
plot(bla,mixtures$CompressiveStrength)
points(bla[mixtures$Age < 5],mixtures$CompressiveStrength[mixtures$Age < 5],col="red",pch=16)
points(bla[mixtures$Age > 30],mixtures$CompressiveStrength[mixtures$Age >30],col="blue",pch=16)
points(bla[mixtures$Age > 50],mixtures$CompressiveStrength[mixtures$Age >50],col="gold",pch=16)
plot(bla,mixtures$CompressiveStrength)
points(bla[mixtures$Age < 5],mixtures$CompressiveStrength[mixtures$Age < 5],col="red",pch=16)
points(bla[mixtures$Age > 30],mixtures$CompressiveStrength[mixtures$Age >30],col="blue",pch=16)
points(bla[mixtures$Age > 100],mixtures$CompressiveStrength[mixtures$Age >100],col="gold",pch=16)
plot(bla,mixtures$CompressiveStrength)
points(bla[mixtures$Age < 5],mixtures$CompressiveStrength[mixtures$Age < 5],col="red",pch=16)
points(bla[mixtures$Age > 10],mixtures$CompressiveStrength[mixtures$Age >10],col="pink",pch=16)
points(bla[mixtures$Age > 30],mixtures$CompressiveStrength[mixtures$Age >30],col="blue",pch=16)
points(bla[mixtures$Age > 100],mixtures$CompressiveStrength[mixtures$Age >100],col="gold",pch=16)
hist(mixtures$FlyAsh)
plot(bla,mixtures$CompressiveStrength)
points(bla[mixtures$FlyAsh < 0.01],mixtures$CompressiveStrength[mixtures$FlyAsh < 0.01],col="red",pch=16)
points(bla[mixtures$FlyAsh > 0.04],mixtures$CompressiveStrength[mixtures$FlyAsh >0.04],col="pink",pch=16)
points(bla[mixtures$FlyAsh > 0.06],mixtures$CompressiveStrength[mixtures$FlyAsh >0.06],col="blue",pch=16)
points(bla[mixtures$FlyAsh > 0.08],mixtures$CompressiveStrength[mixtures$FlyAsh >0.08],col="gold",pch=16)
hist(mixtures$SuperPlasticizer)
hist(as.numeric(mixtures$SuperPlasticizer))
mixtures$SuperPlasticizer
mixtures$Superplasticizer
hist(mixtures$Superplasticizer)
log(mixtures$Superplasticizer)
hist(log(mixtures$Superplasticizer+1))
hist(mixtures$Superplasticizer)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
AlzheimerDisease
data(AlzheimerDisease)
AlzheimerDisease
summary(AlzheimerDisease)
library(AppliedPredictiveModeling)
adData
sumamry(adData)
summary(adData)
colnames(adData)
colnames(adData[,58:69])
CR.pca <- prcomp(adData[,58:69], scale = TRUE)
summary(CR.pca)
fit1 <-train(diagnosis ~ adData[,58:69],method="glm",preProcess="pca",pcaComp=7)
fit1 <-train(diagnosis ~ as.data.frame(adData[,58:69]),method="glm",preProcess="pca",pcaComp=7)
as.data.frame(adData[,58:69])
colnames(adData[,58:69])
adData[1:2,58:69]
bla <- as.data.frame(adData[,58:69])
fit1 <-train(diagnosis ~ bla,method="glm",preProcess="pca",pcaComp=7)
bla <- data.frame(adData[,58:69])
fit1 <-train(diagnosis ~ bla,method="glm",preProcess="pca",pcaComp=7)
head(bla)
fit1 <-train(diagnosis ~ bla,method="glm",preProcess="pca",pcaComp=7,data=adData)
summary(bla)
fit1 <-train(training$diagnosis ~ training[,58:69],method="glm",preProcess="pca",pcaComp=7,data=adData)
head(training)
training2 <- training[,58:69]
fit1 <-train(training$diagnosis ~ training2,method="glm",preProcess="pca",pcaComp=7)
fit1 <-train(training$diagnosis ~ training$IL_11 + training$IL_13 +training$IL_16 +training$IL_17E +training$IL_1alpha +training$IL_3 +training$IL_4 +training$IL_5 +training$IL_6 +training$IL_6_Receptor +training$IL_7 +training$IL_8,
method="glm",preProcess="pca",pcaComp=7)
fit1
CR.pca <- prcomp(training[,58:69], scale = TRUE)
summary(CR.pca)
preProc <- preProcess(training[,58:69],method="pca",pcaComp=7)
preProc <- preProcess(training[,58:69],method="pca",pcaComp=7)
training2 <- predict(preProc,training[,58:69]))
fit1 <-train(training$diagnosis ~ .,
method="glm",preProcess="pca",data=training2)
fit1
testing2 <- predict(preProc,testing[,58:69]))
confusionMatrix(testing$diagnosis,predict(fit1,testing2))
testing2 <- predict(preProc,testing[,58:69])
confusionMatrix(testing$diagnosis,predict(fit1,testing2))
colnames(testing[,58:69])
names(testing2)
preProc <- preProcess(training[,58:69],method="pca",pcaComp=7)
training2 <- predict(preProc,training[,58:69]))
fit1 <-train(training$diagnosis ~ .,
method="glm",preProcess="pca",data=training2)
testing2 <- predict(preProc,testing[,58:69])
confusionMatrix(testing$diagnosis,predict(fit1,testing2))
preProc
fit1 <-train(training$diagnosis ~ .,
method="glm",preProcess="pca",data=training2, method="pca",trControl = trainControl(preProcOptions = list(thresh = 0.8))
confusionMatrix(testing$diagnosis,predict(fit1,testing))
fit1 <-train(training$diagnosis ~ .,
method="glm",preProcess="pca",data=training2, method="pca",trControl = trainControl(preProcOptions = list(thresh = 0.8)))
fit1 <-train(training$diagnosis ~ .,
method="glm",preProcess="pca",data=training2, trControl = trainControl(preProcOptions = list(thresh = 0.8)))
confusionMatrix(testing$diagnosis,predict(fit1,testing))
fit1 <-train(training$diagnosis ~ .,
method="glm",preProcess="pca",data=training, trControl = trainControl(preProcOptions = list(thresh = 0.8)))
confusionMatrix(testing$diagnosis,predict(fit1,testing))
fit2 <-train(training$diagnosis ~ .,
method="glm",data=training)
confusionMatrix(testing$diagnosis,predict(fit2,testing))
training2 <- training[,58:69]
testing2 <- testing[,58:69]
training2 <- training[,58:69]
testing2 <- testing[,58:69]
fit1 <-train(training$diagnosis ~ .,
method="glm",preProcess="pca",data=training2, trControl = trainControl(preProcOptions = list(thresh = 0.8)))
confusionMatrix(testing$diagnosis,predict(fit1,testing2))
fit2 <-train(training$diagnosis ~ .,
method="glm",data=training2)
confusionMatrix(testing$diagnosis,predict(fit2,testing2))
setwd('/Users/benayoun/Dropbox/Coursera_data_science/practical_machine_learning/')
library('caret')
setwd('/Users/benayoun/Dropbox/Coursera_data_science/practical_machine_learning/')
library('caret')
my.notUsable <- rep(FALSE,160)
for (i in 1:160) {
if (sum(my.har.training[,i] %in% NA) > 19000) {
my.notUsable[i] <- TRUE
}
if (sum(my.har.training[,i] %in% "") > 19000) {
my.notUsable[i] <- TRUE
}
if (sum(my.har.training[,i] %in% "#DIV/0!") > 19000) {
my.notUsable[i] <- TRUE
}
}
sum(my.notUsable) # 100
my.har.training <- read.csv('pml-training.csv',header=TRUE)
my.har.testing <- read.csv('pml-testing.csv',header=TRUE)
summary(my.har.training)
# 160 variables
my.notUsable <- rep(FALSE,160)
for (i in 1:160) {
if (sum(my.har.training[,i] %in% NA) > 19000) {
my.notUsable[i] <- TRUE
}
if (sum(my.har.training[,i] %in% "") > 19000) {
my.notUsable[i] <- TRUE
}
if (sum(my.har.training[,i] %in% "#DIV/0!") > 19000) {
my.notUsable[i] <- TRUE
}
}
sum(my.notUsable) # 100
summary(my.har.training[,-which(my.notUsable)])
pairs(my.har.training[,-which(my.notUsable)])
?pairs
pairs(data.frame(my.har.training[,-which(my.notUsable)]))
?train
set.seed(123456)
my.rf.fit <-train(classe ~ ., method="rf",data=my.har.training[,-which(my.notUsable)],
importance=TRUE,trControl = trainControl(method = "cv", number = 10), )
varImp(my.rf.fit, type=2)
set.seed(123456)
my.rf.fit <-train(my.har.training$classe ~ ., method="rf",data=my.har.training[,-which(my.notUsable)],
importance=TRUE,trControl = trainControl(method = "cv", number = 10), )
my.clean.training <- data.frame(my.har.training[,-which(my.notUsable)])
summary(my.clean.training)
set.seed(123456)
my.rf.fit <-train(my.clean.training$classe ~ ., method="rf",data=my.clean.training,
importance=TRUE,trControl = trainControl(method = "cv", number = 10), )
?train
?rf
?randomForest
my.rf.fit <-train(my.clean.training$classe ~ ., method="rf",data=my.clean.training,
importance=TRUE)
my.clean.training <- data.frame(my.har.training[,-which(my.notUsable)])
set.seed(123456)
my.ctrl.opt <-trainControl(method = "cv", number = 10)
my.rf.fit <-train(my.clean.training$classe ~ ., method="rf",data=my.clean.training,
importance=TRUE,trControl = my.ctrl.opt)
my.final.training <- createDataPartition(my.clean.training$classe, p=0.5, list=FALSE)
har.training <- my.clean.training[my.final.training,]
har.testing <- my.clean.training[-my.final.training,]
set.seed(123456)
my.ctrl.opt <-trainControl(method = "cv", number = 10)
my.rf.fit <-train(har.training$classe ~ ., method="rf",data=har.training,
importance=TRUE,trControl = my.ctrl.opt)
set.seed(123456)
my.ctrl.opt <-trainControl(method = "cv", number = 2)
my.rf.fit <-train(har.training$classe ~ ., method="rf",data=har.training,
importance=TRUE,trControl = my.ctrl.opt)
varImp(my.rf.fit, type=2)
my.clean.training <- data.frame(my.har.training[,-c(1,2,(which(my.notUsable))])
my.final.training <- createDataPartition(my.clean.training$classe, p=0.5, list=FALSE)
har.training <- my.clean.training[my.final.training,]
har.testing <- my.clean.training[-my.final.training,]
set.seed(123456)
my.ctrl.opt <-trainControl(method = "cv", number = 2)
my.rf.fit <-train(har.training$classe ~ ., method="rf",data=har.training,
importance=TRUE,trControl = my.ctrl.opt)
summary(my.clean.training)
colnames(my.clean.training)
my.clean.training <- data.frame(my.har.training[,-c(1,2,(which(my.notUsable))])
my.clean.training <- data.frame(my.har.training[,-c(1,2,which(my.notUsable))])
colnames(my.clean.training)
my.clean.training <- data.frame(my.har.training[,-c(which(my.notUsable))])
my.clean.training <- data.frame(my.har.training[,-c(which(my.notUsable))])
colnames(my.clean.training)
my.clean.training <- my.clean.training[,-1:5]
my.clean.training <- my.clean.training[,-(1:5)]
my.final.training <- createDataPartition(my.clean.training$classe, p=0.5, list=FALSE)
har.training <- my.clean.training[my.final.training,]
har.testing <- my.clean.training[-my.final.training,]
set.seed(123456)
my.ctrl.opt <-trainControl(method = "cv", number = 5)
my.rf.fit <-train(har.training$classe ~ ., method="rf",data=har.training,
importance=TRUE,trControl = my.ctrl.opt)
my.zeroVar <-  nearZeroVar(my.clean.training[sapply(my.clean.training, is.numeric)], saveMetrics = TRUE)
my.zeroVar
my.clean.training <- my.clean.training[,-(1:5)]
# "raw_timestamp_part_1" "raw_timestamp_part_2" "cvtd_timestamp" -> not relevant
#1,2: X and people name should not be used for a general model
my.final.training <- createDataPartition(my.clean.training$classe, p=0.5, list=FALSE)
har.training <- my.clean.training[my.final.training,]
har.testing <- my.clean.training[-my.final.training,]
my.final.training <- createDataPartition(my.clean.training$classe, p=0.75, list=FALSE)
har.training <- my.clean.training[my.final.training,]
har.testing <- my.clean.training[-my.final.training,]
set.seed(123456)
my.ctrl.opt <-trainControl(method = "cv", number = 5)
my.rf.fit <-train(har.training$classe ~ ., method="rf",data=har.training,
importance=TRUE,trControl = my.ctrl.opt)
varImp(my.rf.fit, type=2)
my.rf.preds <- predict(my.rf.fit, har.testing)
confusionMatrix(har.testing$classe, my.rf.preds)
my.confus.mat <- confusionMatrix(har.testing$classe, my.rf.preds)
my.confus.mat$table
my.confus.mat$overall
varImpPlot(my.rf.fit$finalModel, sort = TRUE, type = 1, pch = 16, col = "red", cex = 1, main = "Variable importance")
?varImpPlot
varImpPlot(my.rf.fit$finalModel, sort = TRUE, type = 1, pch = 16, bg = "red", cex = 1, main = "Variable importance")
varImpPlot(my.rf.fit$finalModel, sort = TRUE, type = 1, pch = 16, bg = "red", cex = 1, main = "Variable importance")
my.clean.testing <- data.frame(my.har.testing[,-c(which(my.notUsable))])
my.clean.testing <- my.clean.testing[,-(1:5)]
my.test.preds <- predict(my.rf.fit, my.clean.testing)
my.test.preds
my.test.preds <- as.character(my.test.preds)
my.test.preds
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
pml_write_files(my.test.preds)
my.test.preds <- predict(my.rf.fit, my.clean.testing)
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
pml_write_files(my.test.preds)
my.test.preds <- predict(my.rf.fit, my.clean.testing)
my.test.preds <- as.character(my.test.preds)
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
pml_write_files(my.test.preds)
my.rf.fit$finalModel
my.rf.fit$finalModel$err.rate
my.rf.fit$finalModel
my.rf.fit$finalModel$oob.times
my.rf.fit$finalModel
my.rf.fit$finalModel$type
my.rf.fit$finalModel$confusion
my.rf.fit$finalModel$inbag
my.rf.fit$finalModel$forest
plot(my.rf.fit$finalModel)
my.rf.fit$finalModel
my.rf.fit
my.rf.fit$finalModel$err.rate
my.rf.fit$finalModel
my.rf.fit$finalModel$ntree
my.rf.fit$finalModel$err.rate
my.rf.fit$finalModel$err.rate[500]
my.rf.fit$finalModel
plot(my.rf.fit$finalModel)
my.confus.mat$overall
head(summary(my.har.training)
)
dim(my.har.training)
corplot(my.clean.training)
corrplot(my.clean.training)
pairs(my.clean.training)
cor(my.clean.training[,-50])
pheatmap(cor(my.clean.training[,-50]))
library('pheatmap')
pheatmap(cor(my.clean.training[,-50]))
levelplot(cor(my.clean.training[,-50]))
dev.off()
dev.off()
levelplot(cor(my.clean.training[,-50]))
pheatmap(cor(my.clean.training[,-50]))
my.confus.mat$overall
my.confus.mat$overall
my.confus.mat$overall[1]
varImpPlot(my.rf.fit$finalModel, sort = TRUE, type = 1, pch = 16, bg = "red", cex = 1, main = "Variable importance in model")
sum(my.notUsable)
dim(my.clean.training)
my.clean.training <- data.frame(my.har.training[,-c(which(my.notUsable))])
colnames(my.clean.training)
my.clean.training <- my.clean.training[,-(1:5)]
dim(my.clean.training)
pheatmap(cor(my.clean.training[,-55]))
cor(my.clean.training[,-55])
my.clean.training[,-55]
colnames(my.clean.training)
pheatmap(cor(my.clean.training[,-c(55)]))
data.frame(cor(my.clean.training[,-c(55)]))
cor(data.frame(my.clean.training[,-c(55)]))
cor(as.vector(my.clean.training[,-c(55)]))
cor(as.matrix(my.clean.training[,-c(55)]))
typeof(my.clean.training)
typeof(my.har.training)
setwd('/Users/benayoun/Dropbox/Coursera_data_science/practical_machine_learning/')
library('caret')
my.har.training <- read.csv('pml-training.csv',header=TRUE)
my.har.testing <- read.csv('pml-testing.csv',header=TRUE)
summary(my.har.training)
typeof(my.har.training)
my.notUsable <- rep(FALSE,160)
for (i in 1:160) {
if (sum(my.har.training[,i] %in% NA) > 19000) {
my.notUsable[i] <- TRUE
}
if (sum(my.har.training[,i] %in% "") > 19000) {
my.notUsable[i] <- TRUE
}
if (sum(my.har.training[,i] %in% "#DIV/0!") > 19000) {
my.notUsable[i] <- TRUE
}
}
sum(my.notUsable) # 100
summary(my.har.training[,-which(my.notUsable)])
###pairs(data.frame(my.har.training[,-which(my.notUsable)]))
# maybe correl, but keep for interpretability ?
my.clean.training <- data.frame(my.har.training[,-c(which(my.notUsable))])
colnames(my.clean.training)
my.clean.training <- my.clean.training[,-(1:5)]
library('pheatmap')
pheatmap(cor(my.clean.training[,-c(55)]))
pheatmap(cor(matrix(my.clean.training[,-c(55)])))
summary(my.clean.training)
?
cor
summary(my.clean.training[,-55])
my.clean.training[,55]
my.clean.training[,-55]
my.clean.training[,-55]+0
cor(my.clean.training[,-55]+0)
pheatmap(cor(my.clean.training[,-55]+0))
colnames(my.clean.training)
my.clean.training <- data.frame(my.har.training[,-c(which(my.notUsable))])
colnames(my.clean.training)
my.clean.training <- my.clean.training[,-(1:7)]
pheatmap(cor(my.clean.training[,-55]+0))
pheatmap(cor(my.clean.training[,-53]+0))
?sprintf
sprintf("%.0f%% said yes (out of a sample of size %.0f)", 66.666, 3)
```{r, fig.width=8, fig.height = 2, cache=FALSE}
