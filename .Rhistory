render("/Users/benayoun/Dropbox/Coursera_data_science/rmd_stat_inference/07_Asymptopia.Rmd", "/Users/benayoun/Dropbox/Coursera_data_science/rmd_stat_inference/07_Asymptopia.pdf")
library(rmarkdown)
render("/Users/benayoun/Dropbox/Coursera_data_science/rmd_stat_inference/07_Asymptopia.Rmd", "/Users/benayoun/Dropbox/Coursera_data_science/rmd_stat_inference/07_Asymptopia.pdf")
install.packages(pandoc)
install.packages("pandoc")
my.data <- read.table("/Volumes/MyBook_2/Genome_suite/Annotations_MAKER/August6_Maker_gene_models_REPEATS_full_match.gff3",header=F,sep="\t")
head(my.data)
my.length <- my.data$V5-my.data$V4
hist(my.length)
sum(my.length)
sum(my.length)/1023205147
z.test
mean(mtcars$mpg)
qnorm(0.05)
sd(mtcars$mpg)
qnorm(0.05)*sd(mtcars$mpg)/sqrt(length(mtcars$mpg))  + mean(mtcars$mpg)
-qnorm(0.05)*sd(mtcars$mpg)/sqrt(length(mtcars$mpg))  + mean(mtcars$mpg)
my.4 <- which(mtcars$cyl == 4)
my.6 <- which(mtcars$cyl == 6)
t.test(mtcars$mpg[my.4],mtcars$mpg[my.6])
qnorm(0.975)
3 + c(-1,1)*qnorm(0.975)*1.1
3 + c(-1,1)*qnorm(0.025)*1.1
3 + c(-1,1)*qnorm(0.975)*1.1/sqrt(100)
?pbinom
pbinom(55,100,0.5)
pbinom(55,100,0.5,lower.tail = FALSE)
pbinom(54,100,0.5,lower.tail = FALSE)
?pois
?ppois
ppois(30/15800,520,lower.tail=False)
ppois(30/15800,520,lower.tail=F)
ppois(15800,520*30,lower.tail=F)
ppois(15800-1,520*30,lower.tail=F)
bas <- c(140,138,150,148,135)
treat <- c(132,135,151,146,130)
t.test(bas,treat,paired=T)
1100+c(-1,1)*30/sqrt(9)
1100+c(-1,1)*qnorm(0.975)*30/sqrt(9)
1100+c(-1,1)*qnorm(0.95)*30/sqrt(9)
1100+c(-1,1)*qt(0.975)*30/sqrt(9)
1100+c(-1,1)*qt(0.975,df=8)*30/sqrt(9)
1100+c(-1,1)*qt(0.975,df=8)*30
mean(bas)
mean(treat)
t.test(bas,treat)
t.test(bas)
sd(bas)
mean(bas)+c(-1,1)*qt(0.975,df=4)*sd(bas)/sqrt(5)
1100+c(-1,1)*qt(0.975,df=8)*30/sqrt(9)
?pbino
?pbinom
pbinom(2,4,0.5,lower.tail = FALSE)
ppois(10,0.01*1787)
sp <- sqrt( ( (9-1)*1.5^2 + (9-1)*1.8^2 )/(9+9-2)  )
sp
pt(4.419)
(-3-1)/(sp * sqrt(1/9+1/9))
pt(-5.121475)
pt(-5.121475,df=17)
pt(5.121475,df=17)
(1+3)//(sp * sqrt(1/9+1/9))
(1+3)/(sp * sqrt(1/9+1/9))
qnorm(0.95)
n <- 100
qnorm(0.95)*0.04/sqrt(n)
pnorm(0.006579415, mean = 0.01,sd=0.04/sqrt(n),lower.tail=F)
n <- 100
pnorm(qnorm(0.95)*0.04/sqrt(n), mean = 0.01,sd=0.04/sqrt(n),lower.tail=F)
n<-99
n <- 100
pnorm(qnorm(0.95)*0.04/sqrt(n), mean = 0.01,sd=0.04/sqrt(n),lower.tail=F)
n<-99
pnorm(qnorm(0.95)*0.04/sqrt(n), mean = 0.01,sd=0.04/sqrt(n),lower.tail=F)
n<-1000
pnorm(qnorm(0.95)*0.04/sqrt(n), mean = 0.01,sd=0.04/sqrt(n),lower.tail=F)
n<-200
pnorm(qnorm(0.95)*0.04/sqrt(n), mean = 0.01,sd=0.04/sqrt(n),lower.tail=F)
n<-150
pnorm(qnorm(0.95)*0.04/sqrt(n), mean = 0.01,sd=0.04/sqrt(n),lower.tail=F)
n<-140
pnorm(qnorm(0.95)*0.04/sqrt(n), mean = 0.01,sd=0.04/sqrt(n),lower.tail=F)
n<-135
pnorm(qnorm(0.95)*0.04/sqrt(n), mean = 0.01,sd=0.04/sqrt(n),lower.tail=F)
n<-137
pnorm(qnorm(0.95)*0.04/sqrt(n), mean = 0.01,sd=0.04/sqrt(n),lower.tail=F)
n<-138
pnorm(qnorm(0.95)*0.04/sqrt(n), mean = 0.01,sd=0.04/sqrt(n),lower.tail=F)
pnorm(qnorm(0.9)*0.04/sqrt(n), mean = 0.01,sd=0.04/sqrt(n),lower.tail=F)
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
plot(mixtures$CompressiveStrength)
colnames(mixtures)
summary(mixtures)
summary(mixtures,type="l")
plot(mixtures$CompressiveStrength,type='l')
plot(mixtures$CompressiveStrength,mixtures$FlyAsh)
plot(mixtures$CompressiveStrength,mixtures$Age)
length(mixtures$CompressiveStrength)
bla<-1:length(mixtures$CompressiveStrength)
plot(bla,mixtures$CompressiveStrength)
plot(bla[mixtures$Age < 5],mixtures$CompressiveStrength[mixtures$Age < 5])
plot(bla,mixtures$CompressiveStrength)
points(bla[mixtures$Age < 5],mixtures$CompressiveStrength[mixtures$Age < 5],col="red",pch=16)
points(bla[mixtures$Age > 40],mixtures$CompressiveStrength[mixtures$Age >40],col="blue",pch=16)
plot(bla,mixtures$CompressiveStrength)
points(bla[mixtures$Age < 5],mixtures$CompressiveStrength[mixtures$Age < 5],col="red",pch=16)
points(bla[mixtures$Age > 30],mixtures$CompressiveStrength[mixtures$Age >30],col="blue",pch=16)
points(bla[mixtures$Age > 50],mixtures$CompressiveStrength[mixtures$Age >50],col="gold",pch=16)
plot(bla,mixtures$CompressiveStrength)
points(bla[mixtures$Age < 5],mixtures$CompressiveStrength[mixtures$Age < 5],col="red",pch=16)
points(bla[mixtures$Age > 30],mixtures$CompressiveStrength[mixtures$Age >30],col="blue",pch=16)
points(bla[mixtures$Age > 50],mixtures$CompressiveStrength[mixtures$Age >60],col="gold",pch=16)
plot(bla,mixtures$CompressiveStrength)
points(bla[mixtures$Age < 5],mixtures$CompressiveStrength[mixtures$Age < 5],col="red",pch=16)
points(bla[mixtures$Age > 30],mixtures$CompressiveStrength[mixtures$Age >30],col="blue",pch=16)
points(bla[mixtures$Age > 50],mixtures$CompressiveStrength[mixtures$Age >50],col="gold",pch=16)
plot(bla,mixtures$CompressiveStrength)
points(bla[mixtures$Age < 5],mixtures$CompressiveStrength[mixtures$Age < 5],col="red",pch=16)
points(bla[mixtures$Age > 30],mixtures$CompressiveStrength[mixtures$Age >30],col="blue",pch=16)
points(bla[mixtures$Age > 100],mixtures$CompressiveStrength[mixtures$Age >100],col="gold",pch=16)
plot(bla,mixtures$CompressiveStrength)
points(bla[mixtures$Age < 5],mixtures$CompressiveStrength[mixtures$Age < 5],col="red",pch=16)
points(bla[mixtures$Age > 10],mixtures$CompressiveStrength[mixtures$Age >10],col="pink",pch=16)
points(bla[mixtures$Age > 30],mixtures$CompressiveStrength[mixtures$Age >30],col="blue",pch=16)
points(bla[mixtures$Age > 100],mixtures$CompressiveStrength[mixtures$Age >100],col="gold",pch=16)
hist(mixtures$FlyAsh)
plot(bla,mixtures$CompressiveStrength)
points(bla[mixtures$FlyAsh < 0.01],mixtures$CompressiveStrength[mixtures$FlyAsh < 0.01],col="red",pch=16)
points(bla[mixtures$FlyAsh > 0.04],mixtures$CompressiveStrength[mixtures$FlyAsh >0.04],col="pink",pch=16)
points(bla[mixtures$FlyAsh > 0.06],mixtures$CompressiveStrength[mixtures$FlyAsh >0.06],col="blue",pch=16)
points(bla[mixtures$FlyAsh > 0.08],mixtures$CompressiveStrength[mixtures$FlyAsh >0.08],col="gold",pch=16)
hist(mixtures$SuperPlasticizer)
hist(as.numeric(mixtures$SuperPlasticizer))
mixtures$SuperPlasticizer
mixtures$Superplasticizer
hist(mixtures$Superplasticizer)
log(mixtures$Superplasticizer)
hist(log(mixtures$Superplasticizer+1))
hist(mixtures$Superplasticizer)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
AlzheimerDisease
data(AlzheimerDisease)
AlzheimerDisease
summary(AlzheimerDisease)
library(AppliedPredictiveModeling)
adData
sumamry(adData)
summary(adData)
colnames(adData)
colnames(adData[,58:69])
CR.pca <- prcomp(adData[,58:69], scale = TRUE)
summary(CR.pca)
fit1 <-train(diagnosis ~ adData[,58:69],method="glm",preProcess="pca",pcaComp=7)
fit1 <-train(diagnosis ~ as.data.frame(adData[,58:69]),method="glm",preProcess="pca",pcaComp=7)
as.data.frame(adData[,58:69])
colnames(adData[,58:69])
adData[1:2,58:69]
bla <- as.data.frame(adData[,58:69])
fit1 <-train(diagnosis ~ bla,method="glm",preProcess="pca",pcaComp=7)
bla <- data.frame(adData[,58:69])
fit1 <-train(diagnosis ~ bla,method="glm",preProcess="pca",pcaComp=7)
head(bla)
fit1 <-train(diagnosis ~ bla,method="glm",preProcess="pca",pcaComp=7,data=adData)
summary(bla)
fit1 <-train(training$diagnosis ~ training[,58:69],method="glm",preProcess="pca",pcaComp=7,data=adData)
head(training)
training2 <- training[,58:69]
fit1 <-train(training$diagnosis ~ training2,method="glm",preProcess="pca",pcaComp=7)
fit1 <-train(training$diagnosis ~ training$IL_11 + training$IL_13 +training$IL_16 +training$IL_17E +training$IL_1alpha +training$IL_3 +training$IL_4 +training$IL_5 +training$IL_6 +training$IL_6_Receptor +training$IL_7 +training$IL_8,
method="glm",preProcess="pca",pcaComp=7)
fit1
CR.pca <- prcomp(training[,58:69], scale = TRUE)
summary(CR.pca)
preProc <- preProcess(training[,58:69],method="pca",pcaComp=7)
preProc <- preProcess(training[,58:69],method="pca",pcaComp=7)
training2 <- predict(preProc,training[,58:69]))
fit1 <-train(training$diagnosis ~ .,
method="glm",preProcess="pca",data=training2)
fit1
testing2 <- predict(preProc,testing[,58:69]))
confusionMatrix(testing$diagnosis,predict(fit1,testing2))
testing2 <- predict(preProc,testing[,58:69])
confusionMatrix(testing$diagnosis,predict(fit1,testing2))
colnames(testing[,58:69])
names(testing2)
preProc <- preProcess(training[,58:69],method="pca",pcaComp=7)
training2 <- predict(preProc,training[,58:69]))
fit1 <-train(training$diagnosis ~ .,
method="glm",preProcess="pca",data=training2)
testing2 <- predict(preProc,testing[,58:69])
confusionMatrix(testing$diagnosis,predict(fit1,testing2))
preProc
fit1 <-train(training$diagnosis ~ .,
method="glm",preProcess="pca",data=training2, method="pca",trControl = trainControl(preProcOptions = list(thresh = 0.8))
confusionMatrix(testing$diagnosis,predict(fit1,testing))
fit1 <-train(training$diagnosis ~ .,
method="glm",preProcess="pca",data=training2, method="pca",trControl = trainControl(preProcOptions = list(thresh = 0.8)))
fit1 <-train(training$diagnosis ~ .,
method="glm",preProcess="pca",data=training2, trControl = trainControl(preProcOptions = list(thresh = 0.8)))
confusionMatrix(testing$diagnosis,predict(fit1,testing))
fit1 <-train(training$diagnosis ~ .,
method="glm",preProcess="pca",data=training, trControl = trainControl(preProcOptions = list(thresh = 0.8)))
confusionMatrix(testing$diagnosis,predict(fit1,testing))
fit2 <-train(training$diagnosis ~ .,
method="glm",data=training)
confusionMatrix(testing$diagnosis,predict(fit2,testing))
training2 <- training[,58:69]
testing2 <- testing[,58:69]
training2 <- training[,58:69]
testing2 <- testing[,58:69]
fit1 <-train(training$diagnosis ~ .,
method="glm",preProcess="pca",data=training2, trControl = trainControl(preProcOptions = list(thresh = 0.8)))
confusionMatrix(testing$diagnosis,predict(fit1,testing2))
fit2 <-train(training$diagnosis ~ .,
method="glm",data=training2)
confusionMatrix(testing$diagnosis,predict(fit2,testing2))
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
segmentationOriginal
segmentationOriginal$Case
inTrain = which(segmentationOriginal$Case %in% "Train")
training = segmentationOriginal[ inTrain,]
testing = segmentationOriginal[-inTrain,]
setseed(125)
seed(125)
set.seed(125)
summary(training)
colnames(training)
training$class
training$Class
fit1 <-train(training$class ~ .,method="rpart",data=training)
fit1 <-train(training$Class ~ .,method="rpart",data=training)
plot(fit1$finalModel)
fit1
fit1$finalModel
plot(fit1$finalModel)
library(pgmm)
data(olive)
olive = olive[,-1]
install.package("pgmm")
install.packages("pgmm")
library(pgmm)
data(olive)
olive = olive[,-1]
olive
colnames(olive)
newdata = as.data.frame(t(colMeans(olive)))
newdata
fit1 <-train(olive$Area ~ .,method="rpart",data=training)
fit1 <-train(olive$Area ~ .,method="rpart",data=olive)
fit1
plot(fit1$finalModel)
predict(fit1,newdata=newdata)
fit1
set.seed(125)
fit1 <-train(olive$Area ~ .,method="rpart",data=olive)
plot(fit1$finalModel)
predict(fit1,newdata=newdata)
fit1 <-train(olive$Area ~ .,method="tree",data=olive)
predict(fit1,newdata=newdata)
library(tree)
install.packages('library(tree)')
install.packages('tree')
library(tree)
fit1 <-train(olive$Area ~ .,method="tree",data=olive)
predict(fit1,newdata=newdata)
install.packages("caret")
install.packages("caret")
library('caret')
fit1 <-train(olive$Area ~ .,method="tree",data=olive)
predict(fit1,newdata=newdata)
?tree
fit1 <-tree(olive$Area ~ .,data=olive)
predict(fit1,newdata=newdata)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
install.packages('ElemStatLearn')
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
colnames(trainSA)
fit1 <-train(trainSA$chd ~ .,method="glm",family="binomial",data=trainSA)
fit1 <-train(trainSA$chd ~ trainSA$age + trainSA$alcohol + trainSA$obesity + trainSA$tobacco  + trainSA$typea + trainSA$ldl,
method="glm",family="binomial",data=trainSA)
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
predict(fit1,testSA)
missClass(testSA$chd,predict(fit1,testSA))
missClass(SAheart$chd,predict(fit1,SAheart))
missClass(trainSA$chd,predict(fit1,trainSA))
missClass(testSA$chd,predict(fit1,testSA))
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
?SAheart
?predict
missClass(trainSA$chd,predict(fit1,trainSA,type ="response"))
missClass(testSA$chd,predict(fit1,testSA,type ="response"))
missClass
fit1 <-glm(trainSA$chd ~ trainSA$age + trainSA$alcohol + trainSA$obesity + trainSA$tobacco  + trainSA$typea + trainSA$ldl,
family="binomial",data=trainSA)
missClass(trainSA$chd,predict(fit1,trainSA,type ="response"))
missClass(testSA$chd,predict(fit1,testSA,type ="response"))
fit1 <-train(trainSA$chd ~ trainSA$age + trainSA$alcohol + trainSA$obesity + trainSA$tobacco  + trainSA$typea + trainSA$ldl,
method="glm",family="binomial",data=trainSA)
missClass(trainSA$chd,predict(fit1,trainSA,type ="prob"))
missClass(testSA$chd,predict(fit1,testSA,type ="prob"))
predict(fit1,trainSA,type ="prob")
missClass(trainSA$chd,predict(fit1,trainSA))
missClass(testSA$chd,predict(fit1,testSA))
trainSA$chd
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train
colnames(vowel.train)
set.seed(33833)
fit1 <-train(factor(vowel.train$y) ~ .,
method="rf",data=vowel.train)
varImp(fit1)
varImp(fit1,importance = TRUE)
varImp(fit1,value="gini")
?importance
fit1 <-train(factor(vowel.train$y) ~ .,
method="rf",data=vowel.train,importance=TRUE)
varImp(fit1,value="gini")
factor(vowel.train$y)
varImp(fit1, scale = FALSE)
apply(varImp(fit1, scale = FALSE),1,mean)
varImp(fit1, type=2 )
?varImp
?randomForest
fit2 <-randomForest(vowel.train[11,],factor(vowel.train$y))
importance(fit2)
fit2 <-randomForest(vowel.train[,-1],factor(vowel.train$y))
importance(fit2)
set.seed(33833)
fit2 <-randomForest(vowel.train[,-1],factor(vowel.train$y))
importance(fit2)
sort(importance(fit2))
sort(importance(fit2),index.return=T)
importance(fit2)[sort(importance(fit2),index.return=T)$ix]
rownames(importance(fit2))[sort(importance(fit2),index.return=T)$ix]
rownames(importance(fit2))[sort(importance(fit2),index.return=T,decreasing=T)$ix]
set.seed(33833)
fit1 <-train(factor(vowel.train$y) ~ ., method="rf",data=vowel.train,importance=TRUE)
varImp(fit1, type=2 )
set.seed(33833)
fit1 <-train(factor(vowel.train$y) ~ ., method="rf",data=vowel.train)
fit2 <-train(factor(vowel.train$y) ~ ., method="gbm",data=vowel.train)
confusionMatrix(factor(vowel.test$$y),predict(fit1,vowel.test))
confusionMatrix(factor(vowel.test$y),predict(fit1,vowel.test))
confusionMatrix(factor(vowel.test$y),predict(fit2,vowel.test))
confusionMatrix(predict(fit1,vowel.test),predict(fit2,vowel.test))
set.seed(33833)
fit2 <-train(factor(vowel.train$y) ~ ., method="gbm",data=vowel.train)
confusionMatrix(factor(vowel.test$y),predict(fit2,vowel.test)) #
set.seed(33833)
fit1 <-train(factor(vowel.train$y) ~ ., method="rf",data=vowel.train)
fit2 <-train(factor(vowel.train$y) ~ ., method="gbm",data=vowel.train)
confusionMatrix(factor(vowel.test$y),predict(fit1,vowel.test)) #0.6061
confusionMatrix(factor(vowel.test$y),predict(fit2,vowel.test)) #
0.6061*0.5325
confusionMatrix(predict(fit1,vowel.test),predict(fit2,vowel.test)) #
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
colnames(training)
set.seed(62433)
fit1 <-train(training$diagnosis ~ ., method="rf",data=training)
fit2 <-train(training$diagnosis ~ ., method="gbm",data=training)
fit3 <-train(training$diagnosis ~ ., method="lda",data=training)
pred1 <- predict(fit1,testing)
pred2 <- predict(fit2,testing)
pred3 <- predict(fit3,testing)
confusionMatrix(testing$diagnosis,pred1) #0.6061
confusionMatrix(testing$diagnosis,pred2) #
confusionMatrix(testing$diagnosis,pred3) #
bla.data <- data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
combfit<- train(diagnosis ~., method="rf",data=bla.data)
combpred<- predict(combfit,bla.data)
confusionMatrix(testing$diagnosis,combpred) #
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
fit1 <-train(training$diagnosis ~ ., method="rf",data=training,trControl=trainControl(method="cv"),number=3)
fit2 <-train(training$diagnosis ~ ., method="gbm",data=training)
fit1 <-train(training$diagnosis ~ ., method="rf",data=training)
fit2 <-train(training$diagnosis ~ ., method="gbm",data=training,verbose=F)
fit3 <-train(training$diagnosis ~ ., method="lda",data=training)
pred1 <- predict(fit1,testing)
pred2 <- predict(fit2,testing)
pred3 <- predict(fit3,testing)
confusionMatrix(testing$diagnosis,pred1) #0.7683
confusionMatrix(testing$diagnosis,pred2) #0.7927
confusionMatrix(testing$diagnosis,pred3) #0.7683
confusionMatrix(testing$diagnosis,pred2) #0.7683
confusionMatrix(testing$diagnosis,pred3) #0.7683
bla.data <- data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
combfit<- train(diagnosis ~., method="rf",data=bla.data)
combpred<- predict(combfit,bla.data)
confusionMatrix(testing$diagnosis,combpred) #0.8049
set.seed(62433)
fit1 <-train(training$diagnosis ~ ., method="rf",data=training)
fit2 <-train(training$diagnosis ~ ., method="gbm",data=training,verbose=F)
fit3 <-train(training$diagnosis ~ ., method="lda",data=training)
pred1 <- predict(fit1,testing)
pred2 <- predict(fit2,testing)
pred3 <- predict(fit3,testing)
confusionMatrix(testing$diagnosis,pred1) #0.7805
confusionMatrix(testing$diagnosis,pred2) #0.7683
confusionMatrix(testing$diagnosis,pred3) #0.7683
bla.data <- data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
combfit<- train(diagnosis ~., method="rf",data=bla.data)
combpred<- predict(combfit,bla.data)
confusionMatrix(testing$diagnosis,combpred) #0.7805
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
fit1 <-train(training$diagnosis ~ ., method="rf",data=training)
fit2 <-train(training$diagnosis ~ ., method="gbm",data=training,verbose=F)
fit3 <-train(training$diagnosis ~ ., method="lda",data=training)
pred1 <- predict(fit1,testing)
pred2 <- predict(fit2,testing)
pred3 <- predict(fit3,testing)
confusionMatrix(testing$diagnosis,pred1) #0.7805
confusionMatrix(testing$diagnosis,pred2) #0.7927
confusionMatrix(testing$diagnosis,pred3) #0.7683
bla.data <- data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
combfit<- train(diagnosis ~., method="rf",data=bla.data)
combpred<- predict(combfit,bla.data)
confusionMatrix(testing$diagnosis,combpred) #0.8049
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
setwd('/Users/benayoun/Dropbox/Coursera_data_science/practical_machine_learning/')
my.har.training <- read.csv('pml-training.csv',header=TRUE)
summary(my.har.training)
my.notUsable <- rep(FALSE,160)
for (i in 1:160) {
if (sum(my.har.training[,i] %in% NA) > 19000) {
my.notUsable[i] <- TRUE
}
}
sum(my.notUsable)
summary(my.har.training[-my.notUsable])
library('caret')
summary(my.har.training[,-my.notUsable])
featurePlot(x=my.har.training[,-my.notUsable],y=my.har.training$classe)
featurePlot(x=my.har.training[,-my.notUsable],y=my.har.training$classe,plots="pairs")
my.notUsable <- rep(FALSE,160)
for (i in 1:160) {
if (sum(my.har.training[,i] %in% NA) > 19000) {
my.notUsable[i] <- TRUE
}
if (sum(my.har.training[,i] %in% "") > 19000) {
my.notUsable[i] <- TRUE
}
if (sum(my.har.training[,i] %in% "#DIV/0!") > 19000) {
my.notUsable[i] <- TRUE
}
}
sum(my.notUsable) # 67
summary(my.har.training[,-my.notUsable])
which(my.notUsable)
summary(my.har.training[,-which(my.notUsable)])
featurePlot(x=my.har.training[,-which(my.notUsable)],y=my.har.training$classe,plots="pairs")
pairs(my.har.training[,-which(my.notUsable)])
